{
 "metadata": {
  "name": "",
  "signature": "sha256:7f604172deb22ca47707eae2d1f32c29e3690f7b20732ab3061b07f0b4a90bdd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Acquisition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Choosing a data source"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Over the years, the rock climbing community has amassed a wealth of climbing information through a website called http://www.mountainproject.com. This data includes descriptions and facts about individual routes and bouldering problems, information about different parks and areas, and a massive store of user data in the form of comments, ratings, and suggested difficulty [grades](http://en.wikipedia.org/wiki/Grade_%28climbing%29). This is an impressive [community-driven project](http://www.mountainproject.com/community/) and currently functions as a useful web resource for planning climbing trips and obtaining route details. However, in its current form, much of the richness of this dataset is locked away from the community of users who have built it. CragCrunch aims to be a comprehensive and interactive platform for making the most of this unique data resource.\n",
      "\n",
      "To obtain data for user-specific climbing recommendations, I scraped and parsed HTML content from MountainProject. From the site, I extract three distinct kinds of data:\n",
      "- user data (e.g. which routes different climbers climbed, and their personal ratings)\n",
      "- route attributes (e.g. location, elevation, route length, grade)\n",
      "- unstructured text associated with routes (e.g. comments, descriptions)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Scraping MountainProject.com"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scraping, parsing, and cleaning the data from MP was one of the biggest challenges of the project. I used python library [scrapy](http://scrapy.org/) to build crawlers, scrape html, and parse extracted data, and python library sqlalchemy to save the cleaned data to a mysql database.\n",
      "\n",
      "Scraping the data involved digging into the (not entirely systematic) structure of this website using a variety of features of the scrapy library. I first defined a set of Scrapy items that defined the individual kinds of data and relevant attributes to extract from different pages. I then created a set of crawlers which, given a set of starting URLs, followed links according to specified regex rules, applying different sets of scraping functions depending on the type of page it encountered. These scraping functions used xpath expressions to extract desired data based on position in the html and the extracted/parsed data was used to populate item attributes. I then wrote a set of pipelines which performed initial preprocessing of these items, and saved them to a mysql database.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Scrapy steps"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Define spiders to crawl pages:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
      "from scrapy.contrib.linkextractors import LinkExtractor\n",
      "\n",
      "#use CrawlSpider class to define crawlers, e.g.\n",
      "\n",
      "class ClimbAreaSpider(CrawlSpider):\n",
      "    name = \"mpclimbsareas\"\n",
      "    rules = [Rule(LinkExtractor(allow=['\\/v\\/.+\\/\\d+']), callback='parseclimbsandareas', follow=~cleanup)] #LinkExtractor allows us to specify regex rules for following different links on the page\n",
      "    def __init__(self):\n",
      "        super(ClimbAreaSpider, self).__init__()\n",
      "        self.timeout=timeout\n",
      "        self.allowed_domains = [\"mountainproject.com\"]\n",
      "        self.crawlstarttime=datetime.datetime.now()\n",
      "        if cleanup:\n",
      "            self.start_urls= errorurls\n",
      "        else:\n",
      "            self.start_urls = [\n",
      "                #\"http://www.mountainproject.com/v/new-hampshire/105872225\", #specify the urls we want to start on\n",
      "                \"http://www.mountainproject.com/destinations/\"\n",
      "                ]\n",
      "    \n",
      "    #define a callback to be called on the html \n",
      "    def parseclimbsandareas(self, response): #note this needs to be named something other than parse \n",
      "        #html parsing code here (see step 3)\n",
      "        scrapteditem=Area()\n",
      "        ....\n",
      "        return scrapeditem"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Define scrapy item classes that capture desired data schemas"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#the crawler function above will ultimately return items. before scraping those items, we should think about the data we want to acquire and how we want to structure it. \n",
      "\n",
      "#although this is not going to directly structure the database schema, I'll keep that intended schema in mind\n",
      "\n",
      "from scrapy.item import Item, Field\n",
      "\n",
      "class Area(Item):\n",
      "    url = Field()\n",
      "    name = Field()\n",
      "    area = Field()\n",
      "    areaurl = Field()\n",
      "    description = Field()\n",
      "    elevation = Field()\n",
      "    directions = Field()\n",
      "    maplocation = Field()\n",
      "    mapref = Field()\n",
      "    region = Field()\n",
      "    country = Field()\n",
      "    pageviews = Field()\n",
      "\n",
      "class Climb(Item):\n",
      "    url = Field()\n",
      "    name = Field()\n",
      "    description = Field()\n",
      "    locationdescrip = Field()\n",
      "    protection = Field()\n",
      "    grade = Field()\n",
      "    style = Field()\n",
      "    fa = Field()\n",
      "    submittedby = Field()\n",
      "    length = Field()\n",
      "    pitch = Field()\n",
      "    area = Field()\n",
      "    areaurl = Field()\n",
      "    region = Field()\n",
      "    avgstars = Field()\n",
      "    pageviews = Field()\n",
      "    submittedby = Field()"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Use xpath to select relevant html and add to relevant item classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#the crawler will call the function specified by the callback\n",
      "\n",
      "#within this function, use xpath to select desired portions of text\n",
      "#scrapy has a command line tool to explore correct xpath for extracting desired data\n",
      "#...type scrapy shell <url> in command line\n",
      "#sel.xpath(...) will return extracted contents\n",
      "\n",
      "#for example, this function would parse areas and return an Area item\n",
      "def parseareas(self, response): #note this needs to be named something other than parse\n",
      "        sel = Selector(response)\n",
      "        area=Area()\n",
      "        area['url'] = afterwww(response.url)\n",
      "        area['name']=sel.xpath('//h1[@class=\"dkorange\"]/em/text()').extract()[0]\n",
      "        try:\n",
      "            area['area']=sel.xpath('//span[@itemprop=\"title\"]/text()').extract()[-1]\n",
      "            area['areaurl']='mountainproject.com'+sel.xpath('//a[@itemprop=\"url\"]/@href').extract()[-1]\n",
      "            area['region']=sel.xpath('//span[@itemprop=\"title\"]/text()').extract()[0]\n",
      "        except:\n",
      "            area['area']='World'\n",
      "            area['areaurl']='www.google.com'\n",
      "            area['region']='World'\n",
      "        try:\n",
      "            area['maplocation']=sel.xpath('//td[contains(text(),\"Location:\")]/following-sibling::td[1]/text()').extract()[0]\n",
      "        except:\n",
      "            area['maplocation']='unavailable'\n",
      "        try:\n",
      "            area['mapref']=sel.xpath('//td[contains(text(),\"Location:\")]/following-sibling::td[1]/a/@href').extract()[0]\n",
      "        except:\n",
      "            area['mapref']='unavailable'\n",
      "        try:\n",
      "            area['elevation']=sel.xpath('//td[contains(text(),\"Elevation:\")]/following-sibling::td[1]/text()').extract()[0]\n",
      "        except:\n",
      "            area['elevation']='unavailable'\n",
      "        try:        \n",
      "            area['pageviews']=sel.xpath('//td[contains(text(),\"Page Views\")]/following-sibling::td[1]/text()').extract()[0]   \n",
      "        except:\n",
      "            area['pageviews']='unavailable'\n",
      "        descrip1=sel.xpath('//h3[contains(text(),\"Description\")]/following-sibling::p[1]/text()').extract()\n",
      "        descrip2=sel.xpath('//h3[contains(text(),\"Description\")]/following-sibling::div[1]/text()').extract()\n",
      "        if len(descrip1)>0:\n",
      "            area['description']=[par for par in descrip1 if par!=' ']\n",
      "        elif len(descrip2)>0:\n",
      "            area['description']=[par for par in descrip2 if par!=' ']\n",
      "        else:\n",
      "            area['description']='unavailable'\n",
      "        directions1=sel.xpath('//h3[contains(text(),\"Getting There\")]/following-sibling::p[1]/text()').extract()\n",
      "        directions2=sel.xpath('//h3[contains(text(),\"Getting There\")]/following-sibling::div[1]/text()').extract()\n",
      "        if len(descrip1)>0:\n",
      "            area['directions']=[par for par in directions1 if par!=' ']\n",
      "        elif len(descrip2)>0:\n",
      "            area['directions']=[par for par in directions2 if par!=' ']\n",
      "        else:\n",
      "            area['directions']='unavailable'\n",
      "        for key in area.keys():\n",
      "            try:\n",
      "                area[key]=cleanhtml(area[key])\n",
      "            except:\n",
      "                pass\n",
      "        if area['region']!='International':\n",
      "            area['country']='USA'\n",
      "        else:\n",
      "            area['country']='International'\n",
      "            area['region']=area['name']\n",
      "        return area"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. Use sqlalchemy to create database "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from sqlalchemy import create_engine, Column, Integer, String, Boolean, Date, ForeignKey, Text\n",
      "from sqlalchemy.ext.declarative import declarative_base\n",
      "\n",
      "#create cfg class for initiating database connections\n",
      "class Cfg():\n",
      "    def __init__(self):\n",
      "        self.projectroot = projectroot\n",
      "        self.host = host\n",
      "        self.user = user\n",
      "        self.passwd = passwd\n",
      "        self.dbname = 'dbname'\n",
      "        self.charset = 'utf8'\n",
      "        self.use_unicode = 0\n",
      "        self.clobber = False\n",
      "    pass\n",
      "cfg=Cfg()\n",
      "\n",
      "#connect to the database with sqlalchemy engine\n",
      "engine = create_engine('mysql://%s@%s/%s?charset=%s&use_unicode=%s&passwd=%s' %(cfg.user, cfg.host, cfg.dbname, cfg.charset, cfg.use_unicode, cfg.passwd), pool_recycle=3600)\n",
      "\n",
      "Base = declarative_base()\n",
      "\n",
      "#define tables that correspond to item info... e.g.\n",
      "class AreaTable(Base):\n",
      "    __tablename__ = 'Area'\n",
      "    areaid = Column(Integer, primary_key=True, autoincrement=True)\n",
      "    area = Column(Integer, ForeignKey('Area.areaid'))\n",
      "    name = Column(String(70))\n",
      "    url = Column(String(200))\n",
      "    mainarea = Column(Boolean)\n",
      "    description = Column(Text) #String(15000)\n",
      "    elevation = Column(String(70))\n",
      "    directions = Column(Text) #String(3000)\n",
      "    maplocation = Column(String(70))\n",
      "    mapref = Column(String(70))\n",
      "    region = Column(String(20))\n",
      "    country = Column(String(20))\n",
      "    pageviews = Column(String(30))\n",
      "\n",
      "#sqlalchemy will automatically create sql tables with schema corresponding to these classes\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "5. Use pipelines to feed items through basic preprocessing steps and save them to database with sqlalchemy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#collected items are fed through a series of pipeline functions that get the data into the appropriate format for database storange\n",
      "\n",
      "#for example, star items are collected as lists of all items on the page, but we want each row in the star to be a single climber-climb-rating tuple\n",
      "def makesinglestars(item, dbconn):\n",
      "    if len(item['climb'])==len(item['climblink'])==len(item['starsscore']):\n",
      "        for starn,star in enumerate(item['climb']):\n",
      "            try:\n",
      "                s=Stars()\n",
      "                s['climblink']=item['climblink'][starn]\n",
      "                s['climber']=item['climber']\n",
      "                s['starsscore']=item['starsscore'][starn]\n",
      "                s['url']=item['url']\n",
      "                s['name']=item['name']='%s_%s' %(item['climber'], star)\n",
      "                s['climb']=star\n",
      "                if overwritecheck(dbconn, item.__class__.__name__, s):\n",
      "                    write2db(s, mapping[type(item)], dbconn)\n",
      "            except:\n",
      "                warnings.warn('item %s failed' %star)\n",
      "                logfail(item, s)\n",
      "    else:\n",
      "        warnings.warn(\"item %s failed. unqual lengths\" %star)\n",
      "        logfail(item, t)\n",
      "        \n",
      "#create a database connection\n",
      "class DBConnection():\n",
      "    def __init__(self):\n",
      "        #create engine that Session will use for connection\n",
      "        some_engine = create_engine('mysql://%s@%s/%s?charset=%s&use_unicode=%s&passwd=%s' %(cfg.user, cfg.host, cfg.dbname, cfg.charset, cfg.use_unicode, cfg.passwd), pool_recycle=3600)\n",
      "        Session = sessionmaker(bind=some_engine) # create a configured \"Session\" class  \n",
      "        self.session = Session() # create a Session\n",
      "        \n",
      "#write sqlalchemy objects to the database\n",
      "def addandgetid(newobj, dbconn):\n",
      "    '''adds an object and gets its primary ID'''\n",
      "    dbconn.session.add(newobj)\n",
      "    dbconn.session.commit()\n",
      "    field=[f for f in dir(newobj) if 'id' in f][0]\n",
      "    fid=getattr(newobj, field)\n",
      "    return fid\n",
      "        \n",
      "#log acquisition failures so you can iterate on the scraper and improve coverage\n",
      "def logfail(item, o):\n",
      "    rootdir='/home/amyskerry/Projects/climbrec/mpscraper/log'\n",
      "    with open(os.path.join(rootdir, \"errorlog.txt\"), \"a\") as log:\n",
      "        itemtype=item.__class__.__name__\n",
      "        url=item['url']\n",
      "        try:\n",
      "            oname=o['name']\n",
      "        except:\n",
      "            oname=''\n",
      "        log.write(\"%s: %s (%s) failed .... %s\" % (datetime.datetime.now(), itemtype, url, oname))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The end result of the scraping process was a mysql database with 7 iterrelated tables that form the basis for the subsequent analyses. \n",
      "- climbs: database of individual routes and facts about them (descriptions, average rating, average grade, route length, route style, etc.) \n",
      "- climbers: database of individual climbers and facts about them (gender, age, hobbies) \n",
      "- areas: database of individual climbing - areas (parks or \"crags\"), their locations, descriptions \n",
      "- stars: individual user ratings (1-4 star scale) of the climbs (references climbs and climbers)\n",
      "- grades: individual user grades (yosemite decimal system and USA boulder system) of the climbs (references climbs and climbers) \n",
      "- comments: individual user comments about the climbs (references climbs and climbers) \n",
      "- ticks: individual user \"ticks\" (records of having completed a climb, plus notes) (references climbs and climbers)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}