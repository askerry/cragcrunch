{
 "metadata": {
  "name": "",
  "signature": "sha256:8d42292799f1a70921d10e5ccef67a01719ed719586f9e8cdb996e7559c1c077"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preprocessing scraped data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A fair amount of data cleaning was performed online during the scraping process (see scraper pipelines). However, the data stored in the initial database is still a bit messy and there is work to be done post acquisition. Here, I implement further cleaning/wrangling and reduction to get the data in shape for modeling and visualization. In addition to basic cleaning, I perform a number of preprocessing steps to precompute additional variables of interest (e.g. compute for each climb the delta between each individual users rating and the rating of all other users, get word frequency vectors for a set of a priori domain relevant terms, etc.) so that I can work quickly in the analysis/visualization stage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alreadyprocessed=False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#general\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "np.random.RandomState(100)\n",
      "from sqlalchemy import create_engine\n",
      "import MySQLdb\n",
      "import os\n",
      "import sys\n",
      "sys.setrecursionlimit(3000)\n",
      "\n",
      "#project specific code\n",
      "sys.path.append('mpscraper')\n",
      "from mpscraper.cfgdb import cfg\n",
      "sys.path.append('antools')\n",
      "import antools.randomstuff as rd\n",
      "import antools.utilities as util\n",
      "import antools.clean as clean\n",
      "import antools.reduction as red\n",
      "import antools.miscdf as miscdf\n",
      "import antools.climbingcalcs as ccc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#misc config\n",
      "rootdir=os.getcwd()\n",
      "while 'Projects' in rootdir:\n",
      "    rootdir=os.path.dirname(rootdir)\n",
      "rootdir=os.path.join(rootdir, 'Projects','cragcrunch')\n",
      "dropfulls=True #if true, drop the full dataframes once we split into train and test (to save RAM)\n",
      "writedb=True #write dfs to sql database\n",
      "chunksize=1000 #chunk size when writing out large dataframes to sql\n",
      "terms=rd.rockterms+rd.holdterms+rd.descriptors+rd.easeterms+rd.safetyterms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load in raw scraped data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if alreadyprocessed==False:\n",
      "    con=MySQLdb.connect(user=cfg.user, passwd=cfg.passwd, db=cfg.dbname, host=cfg.host, charset=cfg.charset, use_unicode=cfg.charset)\n",
      "    full_climbdf = pd.read_sql(\"SELECT * from Climb\", con)\n",
      "    full_climbdf=full_climbdf[['name']+[el for el in full_climbdf.columns.tolist() if el!='name']] #moving name first\n",
      "    full_areadf = pd.read_sql(\"SELECT * from Area\", con)\n",
      "    full_climberdf = pd.read_sql(\"SELECT * from Climber\", con)\n",
      "    full_tickdf = pd.read_sql(\"SELECT * from Ticks\", con)\n",
      "    full_commentdf = pd.read_sql(\"SELECT * from Comments\", con)\n",
      "    full_gradedf = pd.read_sql(\"SELECT * from Grades\", con)\n",
      "    full_stardf = pd.read_sql(\"SELECT * from Stars\", con)\n",
      "    full_tododf = pd.read_sql(\"SELECT * from ToDos\", con)\n",
      "    print \"data loaded\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data loaded\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if alreadyprocessed:\n",
      "    engine = create_engine('mysql://%s@%s/%s?charset=%s&use_unicode=%s&passwd=%s' %(cfg.user, cfg.host, cfg.dbname, cfg.charset, cfg.use_unicode, cfg.passwd), pool_recycle=3600)\n",
      "    full_climbdf = pd.read_sql(\"SELECT * from climb_prepped\", engine, index_col='index')\n",
      "    full_areadf = pd.read_sql(\"SELECT * from area_prepped\", engine, index_col='index')\n",
      "    full_climberdf = pd.read_sql(\"SELECT * from climber_prepped\", engine, index_col='index')\n",
      "    full_hitsdf= pd.read_sql(\"SELECT * from hits_prepped\", engine, index_col='index')\n",
      "    full_stardf= pd.read_sql(\"SELECT * from stars_prepped\", engine, index_col='index')\n",
      "    full_gradedf= pd.read_sql(\"SELECT * from grades_prepped\", engine, index_col='index')\n",
      "    full_commentdf= pd.read_sql(\"SELECT * from comments_prepped\", engine, index_col='index')\n",
      "    full_tickdf= pd.read_sql(\"SELECT * from ticks_prepped\", engine, index_col='index')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Clean up individual attributes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "cleandf does some very bare bones cleaning... makes sure columns are of the right datatype, strips empty spaces from ends of strings, cleans up missing values, etc.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "full_climbdf=util.cleandf(full_climbdf)\n",
      "full_areadf=util.cleandf(full_areadf) \n",
      "full_climberdf=util.cleandf(full_climberdf) \n",
      "full_tickdf=util.cleandf(full_tickdf) \n",
      "full_commentdf=util.cleandf(full_commentdf) \n",
      "full_gradedf=util.cleandf(full_gradedf) \n",
      "full_stardf=util.cleandf(full_stardf) \n",
      "full_tododf=util.cleandf(full_tododf) \n",
      "print \"performed dataframe initial cleaning\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "performed dataframe initial cleaning\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Drop climb styles that fall outside scope of the project"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we are focusing on rock climbing and will drop alpine/ice climbing data from the dataset\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "i=len(full_climbdf)\n",
      "full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf=red.dropstyles(full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf)\n",
      "print \"dropped types of climbs we don't care about\"\n",
      "print \"starting with %s, reducing to %s\" %(i, len(full_climbdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dropped types of climbs we don't care about\n",
        "starting with 121849, reducing to 106561\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Limit to climbs and areas in the USA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "i=len(full_areadf)\n",
      "j=len(full_climbdf)\n",
      "full_climbdf, full_areadf=red.limittoUSA(full_climbdf, full_areadf)\n",
      "print \"limited to USA\"\n",
      "print \"starting with %s, reducing to %s\" %(i, len(full_areadf))\n",
      "print \"starting with %s, reducing to %s\" %(j, len(full_climbdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "limited to USA\n",
        "starting with 26703, reducing to 21106\n",
        "starting with 106561, reducing to 98341\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Align location information and drop climbs that are missing location info"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=len(full_climbdf)\n",
      "full_climbdf=pd.merge(full_climbdf, full_areadf[['areaid','region']], left_on='area', right_on='areaid', how='inner')\n",
      "full_climbdf=full_climbdf.rename(columns={'region_y':'region'})\n",
      "del full_climbdf['region_x']\n",
      "print \"added regions to climbs and dropped climbs missing areas\"\n",
      "print \"starting with %s, reducing to %s\" %(i, len(full_climbdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "added regions to climbs and dropped climbs missing areas\n",
        "starting with 98341, reducing to 92314\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Drop climbs for which we don't have grade information\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=len(full_climbdf)\n",
      "full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf=red.dropungraded(full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf)\n",
      "print \"starting with %s, reducing to %s\" %(i, len(full_climbdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "starting with 92314, reducing to 92314\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Removing duplicates, fixing missing values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=len(full_areadf)\n",
      "j=len(full_climbdf)\n",
      "full_climberdf, full_areadf, full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf=util.dedupsandmissing(full_climberdf, full_areadf, full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf)\n",
      "print \"performed deduping, filled in missing values\"\n",
      "print \"starting with %s, reducing to %s\" %(j, len(full_climbdf))\n",
      "print \"starting with %s, reducing to %s\" %(i, len(full_areadf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "performed deduping, filled in missing values\n",
        "starting with 92314, reducing to 92314\n",
        "starting with 21106, reducing to 21005\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create Climber x Climb Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "make single df of all climberxclimb interactions (of any kind: tick, star, grade, comment), on the assumption that any of these indicate an attempt at the climb. (i.e. 1 or 0 based on whether climber interacted with climb in any capacity)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_hitsdf=pd.concat([full_tickdf[['climb','climber', 'urlname']],full_tododf[['climb','climber','urlname']],full_commentdf[['climb','climber','urlname']],full_gradedf[['climb','climber','urlname']]])\n",
      "full_hitsdf=full_hitsdf.drop_duplicates()\n",
      "full_hitsdf=full_hitsdf.loc[~np.isnan(full_hitsdf.climber.values)]\n",
      "full_hitsdf.index=range(len(full_hitsdf))\n",
      "full_hitsdf['hitsid']=full_hitsdf.index.values\n",
      "print \"created full climber x climb matrix\"\n",
      "print \"%s hits\" %len(full_hitsdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "created full climber x climb matrix\n",
        "159807 hits\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Misc fixes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#fix up some climber data errors\n",
      "full_climberdf=clean.fixclimbers(full_climberdf, full_hitsdf)\n",
      "\"fixed duped climbers\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "'fixed duped climbers'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reindex everything so that id is index\n",
      "%autoreload\n",
      "full_hitsdf, full_climberdf, full_areadf, full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf=util.reindexall(full_hitsdf,full_climberdf, full_areadf, full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf)\n",
      "print \"reindexed dataframes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reindexed dataframes\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OOPS. goofed up when extracting data and thought we were in a 1-5 system. actually in 0-4 where zero is weird \"danger\"/\"bomb\" rating that I'll ignore for now"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#random cleanup hacks\n",
      "full_stardf.starsscore=full_stardf.starsscore-1\n",
      "full_climberdf.loc[full_climberdf['age']>120, 'age']=np.nan #catch some jokesters\n",
      "print \"performed random cleanup hacks\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "performed random cleanup hacks\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#limit all dfs to the climbs we currently have in climbdf\n",
      "full_hitsdf, full_stardf, full_commentdf, full_gradedf, full_tickdf=red.dropclimbs(full_climbdf, full_hitsdf, full_stardf, full_commentdf, full_gradedf, full_tickdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Convert grade to a linear scale"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we may want to compute differences between grades (e.g. across users) and therefore need a metric scale for the grade data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climbdf['numerizedgrade']=full_climbdf['grade'].apply(clean.numerizegrades, gradelists=[rd.grades, rd.bouldergrades])\n",
      "full_climbdf['numgrade']=full_climbdf['grade'].apply(clean.splitgrade, output='num')\n",
      "full_climbdf['lettergrade']=full_climbdf['grade'].apply(clean.splitgrade, output='letter')\n",
      "print \"redefined grading\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "redefined grading\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Find main parks/areas for climbs and climbers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states, stateids, statedict, mainareadf, xa, xcl=ccc.getstatesandmainareas(full_areadf, full_climbdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "identified main areas/parks for subareas\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "because we dropped non-USA and certain forms of climbers, there are users for which we have no climbing history. drop them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=len(full_climberdf)\n",
      "full_climberdf=full_climberdf[full_climberdf['climberid'].isin(full_hitsdf.climber.values)]\n",
      "print \"dropping climbers for which we have no climbing history\"\n",
      "print \"starting with %s, reducing to %s\" %(i, len(full_climberdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dropping climbers for which we have no climbing history\n",
        "starting with 5685, reducing to 1535\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm going to assume that the park you climb in most is your home crag/\"mainarea\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=ccc.getclimberareas(full_hitsdf, full_climbdf, full_climberdf) #note this uses whole dataset, shouldn't be part of any training/testing\n",
      "print \"identified climber mainareas\"   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "identified climber mainareas\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getregion(mid, areadf):\n",
      "    return areadf[areadf['areaid']==mid].region.values[0]\n",
      "full_climberdf['region']=full_climberdf['mainarea'].apply(getregion, areadf=full_areadf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extract counts for a priori terms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#combine all comments into single text blob for each climb\n",
      "print \"merging climb comments\"\n",
      "full_climbdf=miscdf.mergeclimbcomment(full_commentdf, full_climbdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "merging climb comments\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "extract word counts for both priori terms (e.g. describing rock types, hold types, face angle, etc) and from comments and descriptions\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allterms=[str(s) for s in util.loadpickledobjects(os.path.join(rootdir,'data', 'allclimbingterms.pkl'))[0]]\n",
      "print \"%s climbing related terms\" %len(allterms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "500 climbing related terms\n"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#merge all text into one blob\n",
      "full_climbdf['mergedtext']=full_climbdf.apply(lambda x:miscdf.mergestrs(x['commentsmerged'],x['description']), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "#using terms from tfidf comparison\n",
      "print \"tokenizing and extracting word frequencies\"\n",
      "#full_climbdf=full_climbdf[[col for col in full_climbdf.columns if '_desc' not in col]]\n",
      "print full_climbdf.shape\n",
      "full_climbdf=miscdf.makedescripdf(full_climbdf, allterms)\n",
      "print \"finished count vectorization\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tokenizing and extracting word frequencies\n",
        "finished count vectorization"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cols=[col for col in full_climbdf.columns if '_d' in col]\n",
      "full_climbdf=util.normalizewordcounts(full_climbdf, cols,'mergedtext')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Eliminate features that are very rare or have very low variance across climbs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "missing1=full_climbdf[cols].describe().loc['mean',:][full_climbdf[cols].describe().loc['mean',:]<.00005].index.values\n",
      "missing1=[m[:m.index('_')] for m in missing1]\n",
      "missing2=full_climbdf[cols].describe().loc['std',:][full_climbdf[cols].describe().loc['std',:]<.0001].index.values\n",
      "missing2=[m[:m.index('_')] for m in missing2]\n",
      "reducedterms=[t for t in allterms if t not in missing1+missing2]\n",
      "print [t for t in terms if t in reducedterms]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "delcols=[c for c in cols if c[:c.index('_')] not in reducedterms]\n",
      "for col in delcols:\n",
      "    del full_climbdf[col]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save set of features\n",
      "util.pickletheseobjects(os.path.join(rootdir,'data', 'climbingterms.pkl'), [reducedterms])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Deal with star rating data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climbdf=clean.fixstars(full_stardf, full_climbdf)\n",
      "print \"fixed stars\"\n",
      "full_climbdf=miscdf.combinestars(full_climbdf)\n",
      "print \"combined stars\"\n",
      "full_climbdf['roundedstars']=full_climbdf['avgstars'].apply(np.round) \n",
      "full_stardf=ccc.computerelativestar(full_stardf,full_climbdf)\n",
      "print \"computed relative stars\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fixed stars\n",
        "combined stars"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computed relative stars"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "morecols=full_climbdf['climbid'].apply(ccc.getratingcounts, sdf=full_stardf)\n",
      "full_climbdf['starcounts']=[x[0] for x in morecols.values]\n",
      "full_climbdf['starstd']=[x[1] for x in morecols.values]\n",
      "print \"counted number of raters for each avg rating score\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "counted number of raters for each avg rating score\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singleids=full_climbdf[full_climbdf.starcounts<=1].climbid.values\n",
      "i=len(full_stardf)\n",
      "full_stardf=full_stardf[~full_stardf['climb'].isin(singleids)]\n",
      "print \"dropping from stardf %s climbs only rated by one climber :(\" %len(singleids)\n",
      "print \"reduced from %s to %s in star df\" %(i, len(full_stardf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dropping from stardf 63809 climbs only rated by one climber :(\n",
        "reduced from 202295 to 179436 in star df\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We at some point may want to use the average rating for a climb as a prior or fallback prediction for an individual climber. but the avg ratings stored in full_climbdf include data from the climber we are trying to predict, which is circular. To deal with this, I compute an \"other_avg\" and \"other_std\" for each climber-climb pair, which is the avg and std of the ratings for all climbers excluding the target."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "othercols=full_stardf.apply(lambda x: ccc.getnonuserstar(x['climb'], x['climber'], sdf=full_stardf[full_stardf['climb']==x['climb']]), axis=1)\n",
      "full_stardf['other_avg']=[x[0] for x in othercols.values]\n",
      "full_stardf['other_std']=[x[1] for x in othercols.values]\n",
      "full_stardf.loc[np.isnan(full_stardf['other_avg']), 'other_avg']=full_climbdf.avgstars.mean()\n",
      "print \"got average rating for each climb-user pair, excluding the rating given by that climber\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "got average rating for each climb-user pair, excluding the rating given by that climber\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#users might differ in their distribution of star ratings, so let's get the mean for each user. we'll do this separately for each rating, excluding the current rating \n",
      "#full_stardf['user_avg']=full_stardf.apply(lambda x:getavg(x['starid'], x['climber'], sdf=full_stardf), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get relative grade"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does the climber find this climb harder than other people who climbed it? This could be informative as it suggests that this climb's attributes are possible weaknesses for the climber..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "#full_gradedf=ccc.computerelativegrades(full_climbdf, full_gradedf)\n",
      "#print \"computed relative grades\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get grade range and average for each climber"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "full_climberdf['numhits']=full_climberdf['climberid'].apply(ccc.getnumhits, hdf=full_hitsdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "cldf = pd.read_sql(\"SELECT * from climber_prepped\", engine, index_col='index')\n",
      "%autoreload\n",
      "for style in ('Sport', 'Trad', 'Boulder'):\n",
      "    ccols=cldf['climberid'].apply(ccc.getclimberdata, style=style, hdf=hdf, cdf=cdf)\n",
      "    cldf['g_min_%s' %style]=[x[0] for x in ccols.values]\n",
      "    cldf['g_max_%s' %style]=[x[1] for x in ccols.values]\n",
      "    cldf['g_median_%s' %style]=[x[2] for x in ccols.values]\n",
      "print \"got some climber grade stats (min, max, median)\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "for style in ('Sport', 'Trad', 'Boulder'):\n",
      "    ccols=full_climberdf['climberid'].apply(ccc.getclimberdata, style=style, hdf=full_hitsdf, cdf=full_climbdf)\n",
      "    full_climberdf['g_min_%s' %style]=[x[0] for x in ccols.values]\n",
      "    full_climberdf['g_max_%s' %style]=[x[1] for x in ccols.values]\n",
      "    full_climberdf['g_median_%s' %style]=[x[2] for x in ccols.values]\n",
      "print \"got some climber grade stats (min, max, median)\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "got some climber grade stats (min, max, median)\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Deal with variability in grading standards across regions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Grading standards can differ across regions. Older trad areas tend to have stiff grades (meaning the climbs are relatively difficult given the grade), whereas newer areas might give the same climb a harder grade. it would be great to be able to take this into account when suggesting climbs well suited to the users climbing level"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##NVM this is actually really hard. regionds differ in stiffness but also in difficulty (e.g. some areas really have only hard climbs, others just grade soft). I'll hold off.\n",
      "#print \"computing grade variability\"\n",
      "#climbdf=computegradevariability(climbdf, gradedf)\n",
      "#climbdf=computeclimbstiffness(climbdf, gradedf, climberdf)\n",
      "#print \"finished grade stiffness\"\n",
      "#areadf=computeareastiffness(climbdf, areadf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#submittercounts=summarizesubmitters(climbdf, climberdf, hitsdf) #note this uses whole dataset, shouldn't be part of any training/testing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Limit to climbs and areas for which we have some user data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we don't have any data of users climbing a climb, let's ignore it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=len(full_areadf)\n",
      "j=len(full_climbdf)\n",
      "full_climbdf=full_climbdf[full_climbdf['climbid'].isin(full_hitsdf.climb.values)]\n",
      "areas=list(full_climbdf.area.unique())+list(full_areadf.area.unique())\n",
      "full_areadf=full_areadf[full_areadf['areaid'].isin(areas)]\n",
      "print \"dropped climbs with no user data\"\n",
      "print \"starting with %s areas, reducing to %s areas\" %(i, len(full_areadf))\n",
      "print \"starting with %s climbs, reducing to %s climbs\" %(j, len(full_climbdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dropped climbs with no user data\n",
        "starting with 21005 areas, reducing to 11918 areas\n",
        "starting with 92314 climbs, reducing to 41473 climbs\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "For now let's limit the product to climbs in California and New Hampshire"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NOTE: I'll do exploratory analyses (visualizing data, playing around with different algorithms and parameters) on the New Hampshire climbs, since I am very familiar with the area and can use my knowledge of the climbs to sanity check my procedures/ qualitatively validate various approaches (in addition to quantititive validation...performance on left out CV folds). Then, I'll perform my final analysis with training and quantitative validation on the California data (the target region for product v1.0). Splitting my analysis into an exploratory stage and a final analysis stage (with separate datasets) allows me to play around with a variety of analytic techniques without overfitting when choosing models, etc. I like to think of all analysis decisions as hyper parameters. Thus, making decisions like choosing a model (e.g. svm vs. logistic regression vs. random forests) because it performs best on your validation data can overestimate your performance in the same way that fitting individual model parameters to your test data would. My approach should allow me to more accurately estimate my ability to predict ratings on unseen data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climbdf=full_climbdf[full_climbdf['region'].isin(['California','New Hampshire', 'World'])]\n",
      "full_areadf=full_areadf[full_areadf['region'].isin(['California','New Hampshire', 'World'])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_hitsdf, full_stardf, full_commentdf, full_gradedf, full_tickdf=red.dropclimbs(full_climbdf, full_hitsdf, full_stardf, full_commentdf, full_gradedf, full_tickdf)\n",
      "full_hitsdf, full_climberdf, full_areadf, full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf, mainareadf=util.renameindices(full_hitsdf, full_climberdf, full_areadf, full_climbdf,full_stardf,full_commentdf,full_gradedf,full_tickdf,full_tododf, mainareadf)\n",
      "full_hitsdf=full_hitsdf[full_hitsdf['climber'].isin(full_climberdf.climberid.unique())]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"%s climbs\" %len(full_climbdf)\n",
      "print \"%s climbers\" %len(full_climberdf)\n",
      "print \"%s areas\" %len(full_areadf)\n",
      "print \"%s hits\" %len(full_hitsdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9600 climbs\n",
        "1535 climbers\n",
        "3315 areas\n",
        "4563 hits\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prejoin a few things to reduce latency of online querying"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climbdf=pd.merge(full_climbdf, full_areadf[['areaid','name']], left_on='mainarea', right_on='areaid', how='left')\n",
      "del full_climbdf['areaid_y']\n",
      "full_climbdf=full_climbdf.rename(columns={'name_x':'name','name_y':'mainarea_name'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climbdf=pd.merge(full_climbdf, full_areadf[['areaid','name']], left_on='area', right_on='areaid', how='left')\n",
      "del full_climbdf['areaid']\n",
      "full_climbdf=full_climbdf.rename(columns={'name_x':'name','name_y':'area_name'})\n",
      "full_climbdf.index=full_climbdf.climbid.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climberdf=full_climberdf[full_climberdf['region'].isin(['New Hampshire', 'California'])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Write preprocessed data to database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if writedb:\n",
      "    #save prepped data to sql\n",
      "    engine = create_engine('mysql://%s@%s/%s?charset=%s&use_unicode=%s&passwd=%s' %(cfg.user, cfg.host, cfg.dbname, cfg.charset, cfg.use_unicode, cfg.passwd), pool_recycle=3600)\n",
      "    full_areadf.to_sql('area_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_climberdf.to_sql('climber_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_tickdf.to_sql('ticks_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_commentdf.to_sql('comments_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_gradedf.to_sql('grades_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_stardf.to_sql('stars_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_tododf.to_sql('todos_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_hitsdf.to_sql('hits_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    full_climbdf.to_sql('climb_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    #regioninfo={'regions':regions, 'regionids':regionids, 'regiondict':regiondict}\n",
      "    #util.pickletheseobjects(os.path.join(rootdir, 'data', 'regioninfo.pkl'), [regioninfo])\n",
      "    util.pickletheseobjects(os.path.join(rootdir, 'data', 'terms.pkl'),[terms])\n",
      "    #util.pickletheseobjects(os.path.join(rootdir, 'data', 'vterms.pkl'),[valenceterms])\n",
      "    mainareadf.to_sql('mainarea_prepped', engine, engine, if_exists='replace', chunksize=chunksize)\n",
      "    #climberxareadf.to_sql('climberxareadf_prepped', engine, engine, if_exists='replace', chunksize=chunksize)\n",
      "    #submittercounts.to_sql('submittercounts_prepped', engine, if_exists='replace', chunksize=chunksize)\n",
      "    print \"wrote to db\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wrote to db\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "DELETE THIS AT THE END"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Last Minute Hacks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- fix jtree\n",
      "- fix or delete royal arches\n",
      "- delete Scirocco, climbid 9636"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "read this:\n",
      "http://www.russpoldrack.org/2012/12/the-perils-of-leave-one-out.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cldf.to_sql('climber_prepped', engine, if_exists='replace', chunksize=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rd.grades[25],rd.grades[28],rd.grades[31]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "('5.6', '5.8', '5.9')"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rd.grades[34],rd.grades[39],rd.grades[46]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "('5.10a', '5.10c', '5.11b')"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rd.bouldergrades[11],rd.bouldergrades[14],rd.bouldergrades[17]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 74,
       "text": [
        "('V3', 'V4', 'V5')"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_climbdf.to_sql('climb_prepped', engine, if_exists='replace', chunksize=500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}